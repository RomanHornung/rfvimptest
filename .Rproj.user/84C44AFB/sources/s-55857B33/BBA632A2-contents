
rm(list=ls());gc()

# setwd("Z:/Projects/BlockForests/Package/blockForest/")
setwd("Z:/Projects/SideProjects/PermVIM/Rpackage/rfvimptest")


library("devtools")

# find_rtools(T)


mydigest <- function (object, algo = c("md5", "sha1", "crc32", "sha256",
                                       "sha512", "xxhash32", "xxhash64", "murmur32"), serialize = TRUE,
                      file = FALSE, length = Inf, skip = "auto", ascii = FALSE,
                      raw = FALSE, seed = 0, errormode = c("stop", "warn", "silent"))
{
  file.access <- R.utils::fileAccess
  algo <- match.arg(algo)
  errormode <- match.arg(errormode)
  .errorhandler <- function(txt, obj = "", mode = "stop") {
    if (mode == "stop") {
      stop(txt, obj, call. = FALSE)
    }
    else if (mode == "warn") {
      warning(txt, obj, call. = FALSE)
      return(invisible(NA))
    }
    else {
      return(invisible(NULL))
    }
  }
  if (is.infinite(length)) {
    length <- -1
  }
  if (is.character(file) && missing(object)) {
    object <- file
    file <- TRUE
  }
  if (serialize && !file) {
    object <- if ("nosharing" %in% names(formals(base::serialize)))
      base::serialize(object, connection = NULL, ascii = ascii,
                      nosharing = TRUE)
    else base::serialize(object, connection = NULL, ascii = ascii)
    if (any(!is.na(pmatch(skip, "auto")))) {
      if (ascii) {
        skip <- which(object[1:30] == as.raw(10))[4]
      }
      else {
        skip <- 14
      }
    }
  }
  else if (!is.character(object) && !inherits(object, "raw")) {
    return(.errorhandler(paste("Argument object must be of type character",
                               "or raw vector if serialize is FALSE"), mode = errormode))
  }
  if (file && !is.character(object))
    return(.errorhandler("file=TRUE can only be used with a character object",
                         mode = errormode))
  algoint <- switch(algo, md5 = 1, sha1 = 2, crc32 = 3, sha256 = 4,
                    sha512 = 5, xxhash32 = 6, xxhash64 = 7, murmur32 = 8)
  if (file) {
    algoint <- algoint + 100
    object <- path.expand(object)
    if (!file.exists(object)) {
      return(.errorhandler("The file does not exist: ",
                           object, mode = errormode))
    }
    if (!isTRUE(!file.info(object)$isdir)) {
      return(.errorhandler("The specified pathname is not a file: ",
                           object, mode = errormode))
    }
    if (file.access(object, 4)) {
      return(.errorhandler("The specified file is not readable: ",
                           object, mode = errormode))
    }
  }
  if (is.character(skip))
    skip <- 0
  val <- .Call(digest_impl, object, as.integer(algoint), as.integer(length),
               as.integer(skip), as.integer(raw), as.integer(seed))
  return(val)
}

library(digest)
R.utils::reassignInPackage("digest", "digest", mydigest)



roxygen2::roxygenise()




x1 = -80538738812075974; y1 = 80435758145817515; z1 = 12602123297335631
x1^3 + y1^3 + z1^3

# check("Z:/Projects/OrdinalForests/Package/ordinalForest")


A <- factor(rep(c(0,1), each=50), levels=c(0,1))
A
B <- rep(NA, 100)
B[A==0] <- rnorm(50)
B[A==1] <- rnorm(50)

y <- factor(rep(0, 100), levels=c(0,1))
y[A==1] <- ifelse(B[A==1] > 0, 1, 0)

y
summary(glm(y ~ A*B, family=binomial))

boxplot(B[A==1] ~ y[A==1])
boxplot(B[A==0] ~ y[A==0])

boxplot(B[A==0 & y==0], B[A==0 & y==1], B[A==1 & y==0], B[A==1 & y==1])


aha2 <- paste("##'   \\item{\\code{", aha, "}}{}", sep="")
for(i in seq(along=aha))
  cat(aha2[i], "\n")



?divfor

?diversityForest


# library(diversityForest)

dim(iris)


iris2 <- iris[sample(1:nrow(iris), size=50),]
table(iris2$Species)
iris2$Species <- as.character(iris2$Species)
iris2 <- iris2[iris2$Species != "setosa",]
iris2$Species <- factor(iris2$Species, levels=c("versicolor", "virginica"))
divfor(Species ~ ., data = iris2, num.trees = 1)


# iris$Species=="setosa"
# iris$Species=="versicolor"
table(iris$Species)







set.seed(1234)
myiris <- iris[c(sample(which(iris$Species=="setosa"), size=10), sample(which(iris$Species=="versicolor"), size=10)),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))

# myiris$Species <- sample(myiris$Species)


# divfor(Species ~ ., data = myiris, num.trees = 1, replace=FALSE, sample.fraction=0.9)
# myiris$Species <- sample(myiris$Species)

was <- divfor(Species ~ ., data = myiris, num.trees = 500, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")
was$variable.importance

was <- interactionfor(Species ~ ., data = myiris, num.trees = 500, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="effect")
was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv


















set.seed(1234)
myiris <- iris[c(which(iris$Species=="setosa"), which(iris$Species=="versicolor")),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))




was <- interactionfor(Species ~ ., data = myiris, num.trees = 30000, replace=FALSE, sample.fraction=0.8)

plot(sort(was$effect.importanceuniv, decreasing=TRUE))
sort(was$effect.importanceuniv, decreasing=TRUE)

plot(sort(was$effect.importancetriv, decreasing=TRUE))
sort(was$effect.importancetriv, decreasing=TRUE)

plot(sort(was$effect.importancebiv, decreasing=TRUE))
sort(was$effect.importancebiv, decreasing=TRUE)


was$prediction.error








was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv





par(mfrow=c(2,1))
plot(was$effect.importanceuniv)
plot(was$effect.importancebiv)
par(mfrow=c(1,1))


was$prediction.error



dim(myiris)




# dim(myiris)
# trainind <- sort(sample(1:nrow(myiris), size=ceiling(nrow(myiris)/2)))
#
# iristrain <- myiris[trainind,]
# iristest <- myiris[setdiff(1:nrow(myiris), trainind),]


was <- interactionfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE)
was$forest$split.types
was$forest$split.multvarIDs

was$prediction.error


was <- divfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE)
was$forest$split.varIDs

was$prediction.error








was$forest$split.multvarIDs

was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv


pred.iris <- predict(was, data = iristest)

















# HIER

set.seed(1234)
myiris <- iris[c(which(iris$Species=="setosa"), which(iris$Species=="versicolor")),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))

trainind <- sort(sample(1:nrow(myiris), size=ceiling(nrow(myiris)/2)))

iristrain <- myiris[trainind,]
iristest <- myiris[setdiff(1:nrow(myiris), trainind),]

iristrain <- iristrain[c(sample(which(iristrain$Species=="setosa"), size=10), sample(which(iristrain$Species=="versicolor"), size=10)),]
iristest <- iristest[c(sample(which(iristest$Species=="setosa"), size=10), sample(which(iristest$Species=="versicolor"), size=10)),]


library("diversityForest")

was <- interactionfor(Species ~ ., data = iristrain, num.trees = 50000, replace=FALSE, sample.fraction=0.8)

pred.iris <- predict(was, data = iristest)

table(pred.iris$predictions, iristest$Species)

ui <- predict(was, data=iristest, predict.all=TRUE)
names(ui)
ui$predictions











# HIERhier

set.seed(1234)
myiris <- iris[c(which(iris$Species=="setosa"), which(iris$Species=="versicolor")),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))

trainind <- sort(sample(1:nrow(myiris), size=ceiling(nrow(myiris)/2)))

iristrain <- myiris[trainind,]
iristest <- myiris[setdiff(1:nrow(myiris), trainind),]

iristrain <- iristrain[c(sample(which(iristrain$Species=="setosa"), size=10), sample(which(iristrain$Species=="versicolor"), size=10)),]
iristest <- iristest[c(sample(which(iristest$Species=="setosa"), size=10), sample(which(iristest$Species=="versicolor"), size=10)),]


library("diversityForest")

was <- interactionfor(Species ~ ., data = iristrain, num.trees = 1, replace=FALSE, sample.fraction=1)

#asdfasdf










was <- divfor(Species ~ ., data = iristrain, num.trees = 3, replace=FALSE, sample.fraction=0.8)





set.seed(1234)
myiris <- iris[c(sample(which(iris$Species=="setosa"), size=10), sample(which(iris$Species=="versicolor"), size=10)),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))

# myiris$Species <- sample(myiris$Species)


# divfor(Species ~ ., data = myiris, num.trees = 1, replace=FALSE, sample.fraction=0.9)
# myiris$Species <- sample(myiris$Species)

was <- divfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")
was$variable.importance

was <- interactionfor(Species ~ ., data = myiris, num.trees = 500, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="effect")
was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv




















library("diversityForest")




set.seed(1234)
myiris <- iris[c(which(iris$Species=="setosa"), which(iris$Species=="versicolor")),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))


dim(myiris)
trainind <- sort(sample(1:nrow(myiris), size=ceiling(nrow(myiris)/2)))

iristrain <- myiris[trainind,]
iristest <- myiris[setdiff(1:nrow(myiris), trainind),]


was <- divfor(dependent.variable.name = "Species", data = iristrain, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")
was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv


pred.iris <- predict(was, data = iristest)


names(pred.iris)
pred.iris$predictions
table(pred.iris$predictions, iristest$Species)

names(was)








# myiris$Species <- sample(myiris$Species)


# divfor(Species ~ ., data = myiris, num.trees = 1, replace=FALSE, sample.fraction=0.9)
# myiris$Species <- sample(myiris$Species)

was <- divfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")
was$variable.importance

was <- interactionfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")
was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv


pred.iris <- predict(rg.iris, data = iris.test)






#asdf




was <- divfor(Species ~ ., data = iris, oob.error=TRUE, importance="permutation")
was$variable.importance

was <- interactionfor(Species ~ ., data = iris, oob.error=TRUE, importance="permutation")
was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv








was <- interactionfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE)
was$variable.importance

was <- interactionfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")
was$effect.importanceuniv
was$effect.importancebiv
was$effect.importancetriv







names(was)



was <- interactionfor(Species ~ ., data = myiris, num.trees = 10, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")


# was <- interactionfor(Species ~ ., data = myiris, num.trees = 10, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")



was <- divfor(Species ~ ., data = myiris, num.trees = 10, replace=FALSE, sample.fraction=0.8, oob.error=TRUE)

was <- divfor(Species ~ ., data = myiris, num.trees = 10, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance="permutation")






set.seed(1234)
myiris <- iris[c(sample(which(iris$Species=="setosa"), size=10), sample(which(iris$Species=="versicolor"), size=10)),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))

# myiris$Species <- sample(myiris$Species)


# divfor(Species ~ ., data = myiris, num.trees = 1, replace=FALSE, sample.fraction=0.9)
# myiris$Species <- sample(myiris$Species)




choose(7, 3)


uiv <- unique(replicate(200000, {
  as.numeric(paste(sort(sample(1:7, size=3)), collapse=""))
}))

sort(uiv)

traceback()




library(diversityForest)

set.seed(1234)
myiris <- iris[c(sample(which(iris$Species=="setosa"), size=10), sample(which(iris$Species=="versicolor"), size=10)),]
myiris$Species <- factor(as.character(myiris$Species), levels=c("setosa", "versicolor"))

myiris$Species <- sample(myiris$Species)

was <- interactionfor(Species ~ ., data = myiris, num.trees = 50, replace=FALSE, sample.fraction=0.8, oob.error=TRUE, importance = "permutation")



## Prediction
train.idx <- sample(nrow(iris), 2/3 * nrow(iris))
iris.train <- iris[train.idx, ]
iris.test <- iris[-train.idx, ]
tuneres <- tunedivfor(formula = Species ~ ., data = iris.train, num.trees.pre = 20)
# NOTE again: num.trees.pre = 20 is specified too small for practical purposes.
rg.iris <- divfor(Species ~ ., data = iris.train, nsplits = 10,
                  proptry = 1, num.trees = 1)
# NOTE again: num.trees = 20 is specified too small for practical purposes.
pred.iris <- predict(rg.iris, data = iris.test)




head(iris2)



myinteractionfor(Species ~ ., data = iris, num.trees = 1)



X <- data.final[, colnames(data.final) != dependent.variable.name]
y <- data.final[, colnames(data.final) == dependent.variable.name]

# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER
# HIER GEHTS WEITER HIER GEHTS WEITER


library(devtools)
find_rtools(T)
install_github("daviddaigithub/BOLTSSIRR")
# (https://github.com/daviddaigithub/BOLTSSIRR).



X <- matrix(nrow=100, ncol=1000, data=rnorm(100*1000))
y <- sample(c(0,1), size=100, replace=TRUE)



getPromispairs <- function(X, y) {

  require("RcppEigen")
  require("BOLTSSIRR")

  # If there are more than 100 observations in the data,
  # they are subset to contain only 100 observations:
  if(nrow(X) > 100) {
    X <- X[sample(1:nrow(X), size=100),]
  }

  # Number of variables:
  p <- ncol(X)

  # If there are fewer than 100 variables, no pre-selection
  # is performed:
  if(p <= 100) {
    promispairs <- t(combn(1:p, 2))
  }
  else {

    # If 100 < p < 448, all pairs are tested for interaction effect
    # and the 5000 pairs with smallest p-values are selected:
    if(p < 448) {

      pairindmat <- t(combn(1:p, 2))

      pval <- 0

      for(i in 1:nrow(pairindmat)) {
        x1 <- X[,pairindmat[i,1]]
        x2 <- X[,pairindmat[i,2]]

        mod <- RcppEigen::fastLmPure(cbind(1, x1, x2, x1*x2), as.numeric(y)-1)
        pval[i] <- 2*(1-pnorm(abs(mod$coefficients[4]), sd=mod$se[4]))
      }

      orderind <- order(pval)[1:5000]
      promispairs <- pairindmat[orderind,]

    }
    else {

      # If 449 < p <= 5000, a combination of the BOLT-SSI procedure and
      # pre-selection using testing is applied:
      if(p <= 5000) {

        # First apply BOLT-SSI to the whole data set:
        model <- BOLT_SSI(X, y, extra_pairs = 5000)

        foundpairs <- model[,1:2]
        foundpairschar <- apply(foundpairs, 1, paste, collapse="_")

        # --> This results in p pre-selected pairs, because
        # BOLT-SSI cannot select more pairs than max{n,p}.

        # Apply BOLT-SSI 20 times to subsets of
        # the variables in order to identify further
        # relevant pairs:

        count <- 1
        while(nrow(foundpairs) < 5000 & count < 20) {

          # Random selection of variables:
          inds <- sort(sample(1:p, size=floor((1/3)*p)))

          # Apply BOLT-SSI:
          model <- BOLT_SSI(X[,inds], y, extra_pairs = 5000)

          foundpairstemp <- model[,1:2]
          foundpairstemp[,1] <- inds[foundpairstemp[,1]]
          foundpairstemp[,2] <- inds[foundpairstemp[,2]]

          foundpairschartemp <- apply(foundpairstemp, 1, paste, collapse="_")

          # Add newly identified promising pairs to the collection
          # of all identified pairs:
          newbool <- !(foundpairschartemp %in% foundpairschar)

          foundpairs <- rbind(foundpairs, foundpairstemp[newbool,])
          foundpairschar <- c(foundpairschar, foundpairschartemp[newbool])

          count <- count + 1
        }


        # If 5000 promising pairs were not yet identified using
        # BOLT-SSI, the remaining pairs are identified by randomly
        # sampling 20 times the number of remaining feature pairs
        # to pre-select, testing each of these and selecting those
        # with smallest p-values:

        nadd <- 20*(5000 - nrow(foundpairs))

        if(nadd <= 0) {

          promispairs <- foundpairs[1:5000,]

          rm(foundpairs);gc()

        } else {

          # Randomly draw feature pairs and exclude those,
          # which already exist in the previously pre-selected
          # pairs:

          pairscand <- matrix(nrow=nadd, ncol=2)
          pairscand[,1] <- sample(1:p, size=nadd, replace=TRUE)
          pairscand[,2] <- sample(1:p, size=nadd, replace=TRUE)

          pairscand <- pairscand[apply(pairscand, 1, function(x) x[1]!=x[2]),]
          pairscand <- t(apply(pairscand, 1, sort))

          pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))
          inclbool <- !duplicated(pairscandstr)

          pairscand <- pairscand[inclbool,]
          pairscandstr <- pairscandstr[inclbool]

          foundpairsstr <- apply(foundpairs, 1, function(x) paste(x, collapse="_"))
          pairscand <- pairscand[sapply(pairscandstr, function(x) !(x %in% foundpairsstr)),]


          # Again draw randomly feature pairs (two times as many, as needed) and exclude
          # already pre-selected ones:

          pairscand2 <- matrix(nrow=2*(nadd - nrow(pairscand)), ncol=2)
          pairscand2[,1] <- sample(1:p, size=2*(nadd - nrow(pairscand)), replace=TRUE)
          pairscand2[,2] <- sample(1:p, size=2*(nadd - nrow(pairscand)), replace=TRUE)

          pairscand2 <- pairscand2[apply(pairscand2, 1, function(x) x[1]!=x[2]),]
          pairscand2 <- t(apply(pairscand2, 1, sort))

          pairscandstr2 <- apply(pairscand2, 1, function(x) paste(x, collapse="_"))
          inclbool <- !duplicated(pairscandstr2)

          pairscand2 <- pairscand2[inclbool,]
          pairscandstr2 <- pairscandstr2[inclbool]

          pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))

          inclbool <- !(pairscandstr2 %in% c(pairscandstr, foundpairsstr))
          pairscand2 <- pairscand2[inclbool,]


          # If enough feature pairs not among the already pre-selected features
          # have been selected, the pre-selection is finished...
          if(nrow(pairscand2) >= nadd - nrow(pairscand)) {
            pairscand <- rbind(pairscand, pairscand2[1:(nadd - nrow(pairscand)),])
          }
          else {
            # ...otherwise we randomly sample feature pairs until enough
            # feature pairs were sampled:
            pairscand <- rbind(pairscand, pairscand2)
            nrest <- nadd - nrow(pairscand)

            pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))

            count <- 0
            while(count < nrest) {
              cand <- sort(sample(1:p, size=2))
              candstr <- paste(cand, collapse="_")
              if(!(candstr %in% pairscandstr)) {
                pairscand <- rbind(pairscand, cand)
                pairscandstr <- c(pairscandstr, candstr)
              }
            }

          }



          # Test each of the sampled feature pairs for interaction
          # effect and keep those with the smallest p-values:

          pval <- 0

          for(i in 1:nrow(pairscand)) {
            x1 <- X[,pairscand[i,1]]
            x2 <- X[,pairscand[i,2]]

            mod <- RcppEigen::fastLmPure(cbind(1, x1, x2, x1*x2), as.numeric(y)-1)
            pval[i] <-2*(1-pnorm(abs(mod$coefficients[4]), sd=mod$se[4]))
          }

          orderind <- order(pval)[1:(nrow(pairscand)/20)]
          pairsrest <- pairscand[orderind,]


          # Finally, add the feature pairs pre-selected using linear regression
          # to those pre-selected using BOLT-SSI:

          foundpairs <- rbind(foundpairs, pairsrest)

          promispairs <- foundpairs

          rm(foundpairs);gc()

        }


      }
      else {

        # If p > 5000, we apply BOLT-SSI once to identify the 5000 most
        # promising feature pairs:

        model <- BOLT_SSI(X,y, extra_pairs = 5000)
        promispairs <- model[,1:2]

      }

    }

  }

  # Return the matrix of the indices of the promising pairs:
  return(promispairs)

}










promispairs <- getPromispairs(X, y)


class(promispairs)

dim(promispairs)

head(promispairs)


promispairs2 <- split(t(promispairs), rep(1:nrow(promispairs), each = ncol(promispairs)))
length(promispairs2)

head(promispairs2)

head(promispairs)


ui <- as.list(promispairs)


length(ui)


head(ui)

# Function to pre-select the pairs of promising variables:

getPromispairs <- function(X, y) {

  require("RcppEigen")
  require("BOLTSSIRR")

  # If there are more than 100 observations in the data,
  # they are subset to contain only 100 observations:
  if(nrow(X) > 100) {
    X <- X[sample(1:nrow(X), size=100),]
  }

  # Number of variables:
  p <- ncol(X)

  # If there are fewer than 100 variables, no pre-selection
  # is performed:
  if(p <= 100) {
    promispairs <- t(combn(1:p, 2))
  }
  else {

    # If 100 < p < 448, all pairs are tested for interaction effect
    # and the 5000 pairs with smallest p-values are selected:
    if(p < 448) {

      pairindmat <- t(combn(1:p, 2))

      pval <- 0

      for(i in 1:nrow(pairindmat)) {
        x1 <- X[,pairindmat[i,1]]
        x2 <- X[,pairindmat[i,2]]

        mod <- RcppEigen::fastLmPure(cbind(1, x1, x2, x1*x2), as.numeric(y)-1)
        pval[i] <- 2*(1-pnorm(abs(mod$coefficients[4]), sd=mod$se[4]))
      }

      orderind <- order(pval)[1:5000]
      promispairs <- pairindmat[orderind,]

    }
    else {

      # If 449 < p <= 5000, a combination of the BOLT-SSI procedure and
      # pre-selection using testing is applied:
      if(p <= 5000) {

        # First apply BOLT-SSI to the whole data set:
        model <- BOLT_SSI(X, y, extra_pairs = 5000)

        foundpairs <- model[,1:2]
        foundpairschar <- apply(foundpairs, 1, paste, collapse="_")

        # --> This results in p pre-selected pairs, because
        # BOLT-SSI cannot select more pairs than max{n,p}.

        # Apply BOLT-SSI 20 times to subsets of
        # the variables in order to identify further
        # relevant pairs:

        count <- 1
        while(nrow(foundpairs) < 5000 & count < 20) {

          # Random selection of variables:
          inds <- sort(sample(1:p, size=floor((1/3)*p)))

          # Apply BOLT-SSI:
          model <- BOLT_SSI(X[,inds], y, extra_pairs = 5000)

          foundpairstemp <- model[,1:2]
          foundpairstemp[,1] <- inds[foundpairstemp[,1]]
          foundpairstemp[,2] <- inds[foundpairstemp[,2]]

          foundpairschartemp <- apply(foundpairstemp, 1, paste, collapse="_")

          # Add newly identified promising pairs to the collection
          # of all identified pairs:
          newbool <- !(foundpairschartemp %in% foundpairschar)

          foundpairs <- rbind(foundpairs, foundpairstemp[newbool,])
          foundpairschar <- c(foundpairschar, foundpairschartemp[newbool])

          count <- count + 1
        }


        # If 5000 promising pairs were not yet identified using
        # BOLT-SSI, the remaining pairs are identified by randomly
        # sampling 20 times the number of remaining feature pairs
        # to pre-select, testing each of these and selecting those
        # with smallest p-values:

        nadd <- 20*(5000 - nrow(foundpairs))

        if(nadd <= 0) {

          promispairs <- foundpairs[1:5000,]

          rm(foundpairs);gc()

        } else {

          # Randomly draw feature pairs and exclude those,
          # which already exist in the previously pre-selected
          # pairs:

          pairscand <- matrix(nrow=nadd, ncol=2)
          pairscand[,1] <- sample(1:p, size=nadd, replace=TRUE)
          pairscand[,2] <- sample(1:p, size=nadd, replace=TRUE)

          pairscand <- pairscand[apply(pairscand, 1, function(x) x[1]!=x[2]),]
          pairscand <- t(apply(pairscand, 1, sort))

          pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))
          inclbool <- !duplicated(pairscandstr)

          pairscand <- pairscand[inclbool,]
          pairscandstr <- pairscandstr[inclbool]

          foundpairsstr <- apply(foundpairs, 1, function(x) paste(x, collapse="_"))
          pairscand <- pairscand[sapply(pairscandstr, function(x) !(x %in% foundpairsstr)),]


          # Again draw randomly feature pairs (two times as many, as needed) and exclude
          # already pre-selected ones:

          pairscand2 <- matrix(nrow=2*(nadd - nrow(pairscand)), ncol=2)
          pairscand2[,1] <- sample(1:p, size=2*(nadd - nrow(pairscand)), replace=TRUE)
          pairscand2[,2] <- sample(1:p, size=2*(nadd - nrow(pairscand)), replace=TRUE)

          pairscand2 <- pairscand2[apply(pairscand2, 1, function(x) x[1]!=x[2]),]
          pairscand2 <- t(apply(pairscand2, 1, sort))

          pairscandstr2 <- apply(pairscand2, 1, function(x) paste(x, collapse="_"))
          inclbool <- !duplicated(pairscandstr2)

          pairscand2 <- pairscand2[inclbool,]
          pairscandstr2 <- pairscandstr2[inclbool]

          pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))

          inclbool <- !(pairscandstr2 %in% c(pairscandstr, foundpairsstr))
          pairscand2 <- pairscand2[inclbool,]


          # If enough feature pairs not among the already pre-selected features
          # have been selected, the pre-selection is finished...
          if(nrow(pairscand2) >= nadd - nrow(pairscand)) {
            pairscand <- rbind(pairscand, pairscand2[1:(nadd - nrow(pairscand)),])
          }
          else {
            # ...otherwise we randomly sample feature pairs until enough
            # feature pairs were sampled:
            pairscand <- rbind(pairscand, pairscand2)
            nrest <- nadd - nrow(pairscand)

            pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))

            count <- 0
            while(count < nrest) {
              cand <- sort(sample(1:p, size=2))
              candstr <- paste(cand, collapse="_")
              if(!(candstr %in% pairscandstr)) {
                pairscand <- rbind(pairscand, cand)
                pairscandstr <- c(pairscandstr, candstr)
              }
            }

          }



          # Test each of the sampled feature pairs for interaction
          # effect and keep those with the smallest p-values:

          pval <- 0

          for(i in 1:nrow(pairscand)) {
            x1 <- X[,pairscand[i,1]]
            x2 <- X[,pairscand[i,2]]

            mod <- RcppEigen::fastLmPure(cbind(1, x1, x2, x1*x2), as.numeric(y)-1)
            pval[i] <-2*(1-pnorm(abs(mod$coefficients[4]), sd=mod$se[4]))
          }

          orderind <- order(pval)[1:(nrow(pairscand)/20)]
          pairsrest <- pairscand[orderind,]


          # Finally, add the feature pairs pre-selected using linear regression
          # to those pre-selected using BOLT-SSI:

          foundpairs <- rbind(foundpairs, pairsrest)

          promispairs <- foundpairs

          rm(foundpairs);gc()

        }


      }
      else {

        # If p > 5000, we apply BOLT-SSI once to identify the 5000 most
        # promising feature pairs:

        model <- BOLT_SSI(X,y, extra_pairs = 5000)
        promispairs <- model[,1:2]

      }

    }

  }

  # Return the matrix of the indices of the promising pairs:
  return(promispairs)

}













# Function to pre-select the pairs of promising variables:

getPromispairs2 <- function(X, y) {

  require("RcppEigen")
  require("BOLTSSIRR")

  # Number of variables:
  p <- ncol(X)

  # If there are fewer than 100 variables, no pre-selection
  # is performed:
  if(p <= 100) {
    promispairs <- t(combn(1:p, 2))
  }
  else {

    # If 100 < p < 448, all pairs are tested for interaction effect
    # and the 5000 pairs with smallest p-values are selected:
    if(p < 448) {

      pairindmat <- t(combn(1:p, 2))

      pval <- 0

      for(i in 1:nrow(pairindmat)) {
        x1 <- X[,pairindmat[i,1]]
        x2 <- X[,pairindmat[i,2]]

        mod <- RcppEigen::fastLmPure(cbind(1, x1, x2, x1*x2), as.numeric(y)-1)
        pval[i] <- 2*(1-pnorm(abs(mod$coefficients[4]), sd=mod$se[4]))
      }

      orderind <- order(pval)[1:5000]
      promispairs <- pairindmat[orderind,]

    }
    else {

      # If 449 < p <= 5000, a combination of the BOLT-SSI procedure and
      # pre-selection using testing is applied:
      if(p <= 5000) {

        # First apply BOLT-SSI to the whole data set:
        model <- BOLT_SSI(X, y, extra_pairs = 5000)

        foundpairs <- model[,1:2]
        foundpairschar <- apply(foundpairs, 1, paste, collapse="_")

        # --> This results in p pre-selected pairs, because
        # BOLT-SSI cannot select more pairs than max{n,p}.

        # Apply BOLT-SSI 20 times to subsets of
        # the variables in order to identify further
        # relevant pairs:

        count <- 1
        while(nrow(foundpairs) < 5000 & count < 20) {

          # Random selection of variables:
          inds <- sort(sample(1:p, size=floor((1/3)*p)))

          # Apply BOLT-SSI:
          model <- BOLT_SSI(X[,inds], y, extra_pairs = 5000)

          foundpairstemp <- model[,1:2]
          foundpairstemp[,1] <- inds[foundpairstemp[,1]]
          foundpairstemp[,2] <- inds[foundpairstemp[,2]]

          foundpairschartemp <- apply(foundpairstemp, 1, paste, collapse="_")

          # Add newly identified promising pairs to the collection
          # of all identified pairs:
          newbool <- !(foundpairschartemp %in% foundpairschar)

          foundpairs <- rbind(foundpairs, foundpairstemp[newbool,])
          foundpairschar <- c(foundpairschar, foundpairschartemp[newbool])

          count <- count + 1
        }


        # If 5000 promising pairs were not yet identified using
        # BOLT-SSI, the remaining pairs are identified by randomly
        # sampling 20 times the number of remaining feature pairs
        # to pre-select, testing each of these and selecting those
        # with smallest p-values:

        nadd <- 20*(5000 - nrow(foundpairs))

        if(nadd <= 0) {

          promispairs <- foundpairs[1:5000,]

          rm(foundpairs);gc()

        } else {

          # Randomly draw feature pairs and exclude those,
          # which already exist in the previously pre-selected
          # pairs:

          pairscand <- matrix(nrow=nadd*1.5, ncol=2)
          pairscand[,1] <- sample(1:p, size=nadd*1.5, replace=TRUE)
          pairscand[,2] <- sample(1:p, size=nadd*1.5, replace=TRUE)

          pairscand <- pairscand[apply(pairscand, 1, function(x) x[1]!=x[2]),]
          pairscand <- t(apply(pairscand, 1, sort))

          pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))
          inclbool <- !duplicated(pairscandstr)

          pairscand <- pairscand[inclbool,]
          pairscandstr <- pairscandstr[inclbool]

          foundpairsstr <- apply(foundpairs, 1, function(x) paste(x, collapse="_"))
          pairscand <- pairscand[sapply(pairscandstr, function(x) !(x %in% foundpairsstr)),]


          # If enough feature pairs not among the already pre-selected features
          # have been selected, the pre-selection is finished...
          if(nrow(pairscand) >= nadd) {
            pairscand <- pairscand[1:nadd,]
          } else {
            # Again draw randomly feature pairs (two times as many, as needed) and exclude
            # already pre-selected ones:

            pairscand2 <- matrix(nrow=2*(nadd - nrow(pairscand)), ncol=2)
            pairscand2[,1] <- sample(1:p, size=2*(nadd - nrow(pairscand)), replace=TRUE)
            pairscand2[,2] <- sample(1:p, size=2*(nadd - nrow(pairscand)), replace=TRUE)

            pairscand2 <- pairscand2[apply(pairscand2, 1, function(x) x[1]!=x[2]),]
            pairscand2 <- t(apply(pairscand2, 1, sort))

            pairscandstr2 <- apply(pairscand2, 1, function(x) paste(x, collapse="_"))
            inclbool <- !duplicated(pairscandstr2)

            pairscand2 <- pairscand2[inclbool,]
            pairscandstr2 <- pairscandstr2[inclbool]

            pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))

            inclbool <- !(pairscandstr2 %in% c(pairscandstr, foundpairsstr))
            pairscand2 <- pairscand2[inclbool,]


            # If enough feature pairs not among the already pre-selected features
            # have been selected, the pre-selection is finished...
            if(nrow(pairscand2) >= nadd - nrow(pairscand)) {
              pairscand <- rbind(pairscand, pairscand2[1:(nadd - nrow(pairscand)),])
            }
            else {
              # ...otherwise we randomly sample feature pairs until enough
              # feature pairs were sampled:
              pairscand <- rbind(pairscand, pairscand2)
              nrest <- nadd - nrow(pairscand)

              pairscandstr <- apply(pairscand, 1, function(x) paste(x, collapse="_"))

              count <- 0
              while(count < nrest) {
                cand <- sort(sample(1:p, size=2))
                candstr <- paste(cand, collapse="_")
                if(!(candstr %in% pairscandstr)) {
                  pairscand <- rbind(pairscand, cand)
                  pairscandstr <- c(pairscandstr, candstr)
                }
              }

            }

          }


          # Test each of the sampled feature pairs for interaction
          # effect and keep those with the smallest p-values:

          pval <- 0

          for(i in 1:nrow(pairscand)) {
            x1 <- X[,pairscand[i,1]]
            x2 <- X[,pairscand[i,2]]

            mod <- RcppEigen::fastLmPure(cbind(1, x1, x2, x1*x2), as.numeric(y)-1)
            pval[i] <-2*(1-pnorm(abs(mod$coefficients[4]), sd=mod$se[4]))
          }

          orderind <- order(pval)[1:(nrow(pairscand)/20)]
          pairsrest <- pairscand[orderind,]


          # Finally, add the feature pairs pre-selected using linear regression
          # to those pre-selected using BOLT-SSI:

          foundpairs <- rbind(foundpairs, pairsrest)

          promispairs <- foundpairs

          rm(foundpairs);gc()

        }


      }
      else {

        # If p > 5000, we apply BOLT-SSI once to identify the 5000 most
        # promising feature pairs:

        model <- BOLT_SSI(X,y, extra_pairs = 5000)
        promispairs <- model[,1:2]

      }

    }

  }

  # Return the matrix of the indices of the promising pairs:
  return(promispairs)

}











X <- matrix(nrow=100, ncol=1000, data=rnorm(100*1000))
y <- sample(c(0,1), size=100, replace=TRUE)


ti1 <- Sys.time()
ah <- getPromispairs(X, y)
ti2 <- Sys.time()
ti2 - ti1




ti1 <- Sys.time()
ah <- getPromispairs2(X, y)
ti2 <- Sys.time()
ti2 - ti1


dim(ah)

uia <- lapply(seq_len(nrow(ah)), function(i) ah[i,])








dim(ah)

head(ah)

length(unique(apply(ah, 1, function(x) paste(x, collapse="_"))))





ah <- getPromispairs(X[,1:300], y)


dim(ah)

head(ah)

length(unique(apply(ah, 1, function(x) paste(x, collapse="_"))))








X <- matrix(nrow=100, ncol=10000, data=rnorm(100*10000))
y <- sample(c(0,1), size=100, replace=TRUE)


ah <- getPromispairs(X, y)


dim(ah)

head(ah)

length(unique(apply(ah, 1, function(x) paste(x, collapse="_"))))



summary(glm(y ~ X[,2125]*X[,6962], family="binomial"))


plot(X[,2125], X[,6962], col=ifelse(y==0, "red", "black"), pch=20, cex=ifelse(y==0, 3, 1))





library("BOLTSSIRR")

model <- BOLT_SSI(X,y, extra_pairs = 5000)

dim(model)

head(model)

class(model2)

dim(model2)






library("BOLTSSIRR")
set.seed(0)
p=500;
n=100;
rho=0.5
H<-abs(outer(1:p,1:p,"-"))
covxx=rho^H
cholcov = chol(covxx)
x0 = matrix(rnorm(n*p), n, p)
x = x0%*%cholcov

#binary response
set.seed(40)
feta = 2*x[,1]+2*x[,8]+3*x[,1]*x[,8];
fprob = exp(feta)/(1+exp(feta))
y = rbinom(n, 1, fprob)

model2=BOLT_SSI(x,y, extra_pairs = 5000)

foundpairs <- model2[,1:2]
foundpairschar <- apply(foundpairs, 1, paste, collapse="_")

npairs <- nrow(foundpairs)

count <- 1
while(nrow(foundpairs) < 5000 & count < 500) {

  if(count %% 2 != 0) {
    inds <- sort(sample(1:ncol(x), size=floor((1/2)*ncol(x))))
    indsafe <- inds
  }
  else {
    inds <- setdiff(1:ncol(x), indsafe)
  }

  model2=BOLT_SSI(x[,inds],y, extra_pairs = 5000)

  foundpairstemp <- model2[,1:2]
  foundpairstemp[,1] <- inds[foundpairstemp[,1]]
  foundpairstemp[,2] <- inds[foundpairstemp[,2]]

  foundpairschartemp <- apply(foundpairstemp, 1, paste, collapse="_")

  newbool <- !(foundpairschartemp %in% foundpairschar)

  foundpairs <- rbind(foundpairs, foundpairstemp[newbool,])
  foundpairschar <- c(foundpairschar, foundpairschartemp[newbool])

  npairs[count] <- nrow(foundpairs)

  cat(paste("indcount:", count), "\n")
  cat(paste("nrow(foundpairs)", nrow(foundpairs)), "\n")

  count <- count + 1
}


plot(npairs)









# Splitting rule. For classification and probability estimation "gini" or "extratrees"
# with default "gini". For regression "variance", "extratrees" or "maxstat" with
# default "variance". For survival "logrank", "extratrees", "C" or "maxstat" with
# default "logrank".



library("diversityForest")

## Set seed to obtain reproducible results:
set.seed(1234)

## Diversity forest with default settings (NOT recommended)

# Classification:
set.seed(1234)
divfor(Species ~ ., data = iris, num.trees = 20)
set.seed(1234)
divfor(Species ~ ., data = iris, num.trees = 20, splitrule="extratrees")
set.seed(1234)
divfor(Species ~ ., data = iris, num.trees = 20, splitrule="extratrees")





# Regression:
iris2 <- iris; iris2$Species <- NULL; iris2$Y <- rnorm(nrow(iris2))
set.seed(1234)
divfor(Y ~ ., data = iris2, num.trees = 20)
set.seed(1234)
divfor(Y ~ ., data = iris2, num.trees = 20, splitrule="maxstat")

# Survival:
library("survival")
divfor(Surv(time, status) ~ ., data = veteran, num.trees = 20, respect.unordered.factors = "order")
# NOTE: num.trees = 20 is specified too small for practical
# purposes, because the prediction performance of the resulting
# forest will be suboptimal!!
# In practice, num.trees = 500 (default value) or a
# larger number should be use







# Check classification case:

head(iris)
iris2 <- iris
# iris2$Species <- NULL
# iris2$Y <- rnorm(nrow(iris2))

set.seed(1234)
ui <- sort(sample(1:nrow(iris2), size=50))
iris2 <- iris2[ui,]


dim(iris2)
dim(iris2)


library("diversityForest")
## Diversity forest with default settings (NOT recommended)
ah <- divfor(Species ~ ., data = iris2, num.trees = 1, verbose=FALSE, nsplits=3, replace=FALSE, sample.fraction=1)


ah


head(iris)

library("diversityForest")
## Diversity forest with default settings (NOT recommended)
ah <- divfor(Species ~ ., data = iris, num.trees = 1, verbose=FALSE, nsplits=3, replace=FALSE, sample.fraction=1)

dim(iris)

ah <- divfor(Species ~ ., data = iris, num.trees = 100, nsplits=30)

head(iris)

sort(unique(iris$Sepal.Length))







# Check regression case:

head(iris)
iris2 <- iris
iris2$Species <- NULL
iris2$Y <- rnorm(nrow(iris2))

head(iris2)

# set.seed(1234)
# ui <- sort(sample(1:nrow(iris2), size=50))
# iris2 <- iris2[ui,]


dim(iris2)
dim(iris2)


library("diversityForest")
## Diversity forest with default settings (NOT recommended)
divfor(Y ~ ., data = iris2, num.trees = 1, verbose=FALSE, nsplits=3, replace=FALSE, sample.fraction=1)


ui <- divfor(Y ~ ., data = iris2, num.trees = 100, verbose=FALSE, nsplits=30)

names(ui)
ui$prediction.error




datareg <- read.table("Z:/Projects/DiversityForests/yacht_hydrodynamics.data")

ui <- divfor(V7 ~ ., data = datareg, num.trees = 100, nsplits=30)


so3 <- function(x) {
  divfor(V7 ~ ., data = datareg, num.trees = 3000, nsplits=x)$prediction.error
}

nsplitseq <- 1:50
errs3 <- sapply(nsplitseq, so3)

plot(nsplitseq, errs3)

plot(nsplitseq[-(1:10)], errs3[-(1:10)])










# Check survival case:

## Survival forest
require(survival)

dim(veteran)
table(veteran$status)

veteran2 <- veteran

# set.seed(1234)
# ui <- sort(sample(1:nrow(veteran2), size=50))
# veteran2 <- veteran2[ui,]

veteran2$celltype <- NULL

library("diversityForest")
ui <- divfor(Surv(time, status) ~ ., data = veteran2, num.trees = 100, verbose=FALSE, nsplits=30)





ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 200, respect.unordered.factors = "order", importance="permutation")


ui$variable.importance



ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 2, respect.unordered.factors = "order")
ui$prediction.error

ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 2)
ui$prediction.error

ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 2, respect.unordered.factors = "partition")
ui$prediction.error




library("diversityForest")

## Set seed to obtain reproducible results:
set.seed(1234)

## Diversity forest with default settings (NOT recommended)
divfor(Species ~ ., data = iris, num.trees = 20)

# getAnywhere("print.divfor")
print.divfor <- function(x, ...) {
  cat("divfor result\n\n")
  cat("Call:\n", deparse(x$call), "\n\n")
  cat("Type:                            ", x$treetype, "\n")
  cat("Number of trees:                 ", x$num.trees, "\n")
  cat("Sample size:                     ", x$num.samples, "\n")
  cat("Number of independent variables: ", x$num.independent.variables, "\n")
  cat("Nsplits:                         ", x$nsplits, "\n")
  cat("Proptry:                         ", x$proptry, "\n")
  cat("Target node size:                ", x$min.node.size, "\n")
  cat("Variable importance mode:        ", x$importance.mode, "\n")
  cat("Splitrule:                       ", x$splitrule, "\n")
  if (x$treetype == "Survival") {
    cat("Number of unique death times:    ", length(x$unique.death.times), "\n")
  }
  if (x$treetype == "Classification") {
    cat("OOB prediction error:            ", sprintf("%1.2f %%", 100*x$prediction.error), "\n")
  } else if (x$treetype == "Regression") {
    cat("OOB prediction error (MSE):      ", x$prediction.error, "\n")
  } else if (x$treetype == "Survival") {
    cat("OOB prediction error (1-C):      ", x$prediction.error, "\n")
  } else if (x$treetype == "Probability estimation") {
    cat("OOB prediction error (Brier s.): ", x$prediction.error, "\n")
  } else {
    cat("OOB prediction error:            ", x$prediction.error, "\n")
  }
  if (x$treetype == "Regression") {
    cat("R squared (OOB):                 ", x$r.squared, "\n")
  }
}
environment(print.divfor) <- environment(divfor)





library("diversityForest")

## Set seed to obtain reproducible results:
set.seed(1234)

## Diversity forest with default settings (NOT recommended)
# Classification:
divfor(Species ~ ., data = iris, num.trees = 20)
# Regression:
iris2 <- iris; iris2$Species <- NULL; iris2$Y <- rnorm(nrow(iris2))
divfor(Y ~ ., data = iris2, num.trees = 20)
# Survival:
library("survival")
divfor(Surv(time, status) ~ ., data = veteran, num.trees = 20, respect.unordered.factors = "order")
# NOTE: num.trees = 20 is specified too small for practical
# purposes, because the prediction performance of the resulting
# forest will be suboptimal!!
# In practice, num.trees = 500 (default value) or a
# larger number should be used.

## Diversity forest with specified values for nsplits and proptry (NOT recommended)
divfor(Species ~ ., data = iris, nsplits = 10, proptry = 0.4, num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.

## Applying diversity forest after optimizing the values of nsplits and proptry (recommended)
tuneres <- tunedivfor(formula = Species ~ ., data = iris, num.trees.pre = 20)
# NOTE: num.trees.pre = 20 is specified too small for practical
# purposes, because the out-of-bag error estimates of the forests
# constructed during optimization will be much too variable!!
# In practice, num.trees.pre = 500 (default value) or a
# larger number should be used.
divfor(Species ~ ., data = iris, nsplits = tuneres$nsplitsopt,
       proptry = tuneres$proptryopt, num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.

## Prediction
train.idx <- sample(nrow(iris), 2/3 * nrow(iris))
iris.train <- iris[train.idx, ]
iris.test <- iris[-train.idx, ]
tuneres <- tunedivfor(formula = Species ~ ., data = iris.train, num.trees.pre = 20)
# NOTE again: num.trees.pre = 20 is specified too small for practical purposes.
rg.iris <- divfor(Species ~ ., data = iris.train, nsplits = tuneres$nsplitsopt,
                  proptry = tuneres$proptryopt, num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.
pred.iris <- predict(rg.iris, data = iris.test)
table(iris.test$Species, pred.iris$predictions)

## Variable importance
rg.iris <- divfor(Species ~ ., data = iris, importance = "permutation", num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.
rg.iris$variable.importance




dim(veteran)



tuneres <- tunedivfor(formula = Surv(time, status) ~ ., data = veteran, num.trees.pre = 20000)
tuneres

ui <- tuneres$tunegrid
ui$err <- tuneres$ooberrs

ui

plot(ui$nsplitsgrid[ui$proptrygrid==0.05], 1-ui$err[ui$proptrygrid==0.05], ylim=range(1-ui$err), ty="o", col=2)
lines(ui$nsplitsgrid[ui$proptrygrid==1], 1-ui$err[ui$proptrygrid==1], ylim=range(ui$err), ty="o")


head(veteran)

sum(apply(veteran[,-c(3,4)], 2, function(x) length(unique(x))-1))

## Prediction
train.idx <- sample(nrow(veteran), 2/3 * nrow(veteran))
iris.train <- veteran[train.idx, ]
iris.test <- veteran[-train.idx, ]
tuneres <- tunedivfor(formula = Surv(time, status) ~ ., data = iris.train, num.trees.pre = 20)
# NOTE again: num.trees.pre = 20 is specified too small for practical purposes.
rg.iris <- divfor(Surv(time, status) ~ ., data = iris.train, nsplits = tuneres$nsplitsopt,
                  proptry = tuneres$proptryopt, num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.
pred.iris <- predict(rg.iris, data = iris.test)
table(iris.test$Species, pred.iris$predictions)



########build_win("Z:/Projects/OrdinalForests/Package/ordinalForest")

# library("devtools")
#
# install("Z:/Projects/BlockForests/Package/blockForest")














parse.formula <- function(formula, data, env = parent.frame()) {
  f <- as.formula(formula)
  t <- terms(f, data = data)

  ## Get dependent var(s)
  response <- data.frame(eval(f[[2]], envir = data, enclos = env))
  colnames(response) <- deparse(f[[2]])

  ## Get independent vars
  independent_vars <- attr(t, "term.labels")
  interaction_idx <- grepl(":", independent_vars)

  ## Error if illegal column name
  if (!all(make.names(independent_vars[!interaction_idx]) == independent_vars[!interaction_idx])) {
    stop("Error: Illegal column names in formula interface. Fix column names or use alternative interface in divfor.")
  }

  ## Shortcut if no interactions
  if (all(!interaction_idx)) {
    return(data.frame(response, data[, independent_vars, drop = FALSE], check.names = FALSE))
  }

  ## Get interaction columns
  if (any(interaction_idx)) {
    interaction_vars <- independent_vars[interaction_idx]
    dat_interaction <- sapply(strsplit(interaction_vars, ":"), function(x) {
      if (any(!sapply(data[, x, drop = FALSE], is.numeric))) {
        stop("Error: Only numeric columns allowed in interaction terms.")
      }
      with(data, eval(parse(text = paste(x, collapse = "*"))))
    })
    colnames(dat_interaction) <- interaction_vars
  }

  ## Get main effect columns
  if (any(!interaction_idx)) {
    main_vars <- independent_vars[!interaction_idx]
    dat_main <- data[, main_vars, drop = FALSE]
  }

  ## Return combined data frame
  if (any(!interaction_idx)) {
    data.frame(response, dat_main, dat_interaction, check.names = FALSE)
  } else {
    data.frame(response, dat_interaction, check.names = FALSE)
  }
}





ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 300, respect.unordered.factors = "order")
ui$prediction.error

ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 300)
ui$prediction.error

# ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 300, respect.unordered.factors = "partition")
# ui$prediction.error






head(veteran)

veteran$celltype

ui <- divfor(Surv(time, status) ~ ., data = veteran, num.trees = 1, verbose=FALSE, replace=FALSE, sample.fraction=1, nsplits=5)

veteranclass <- veteran
head(veteranclass)

veteranclass$Y <- factor(as.numeric(veteranclass$time > median(veteranclass$time)))
veteranclass$time <- veteranclass$status <- NULL

levels(veteranclass$celltype)
sort(sapply(levels(veteranclass$celltype), function(x) mean(veteranclass$Y[veteranclass$celltype==x]==1)))
table(veteranclass$celltype)




library("Matrix")

formula = Y ~ .; data = veteranclass; num.trees = 1; mtry = NULL
importance = "none"; write.forest = TRUE; probability = FALSE
min.node.size = NULL; max.depth = NULL; replace = FALSE
sample.fraction = 1
case.weights = NULL; class.weights = NULL; splitrule = NULL
num.random.splits = 1; alpha = 0.5; minprop = 0.1
split.select.weights = NULL; always.split.variables = NULL
respect.unordered.factors = "partition"
scale.permutation.importance = FALSE
keep.inbag = FALSE; inbag = NULL; holdout = FALSE
quantreg = FALSE; oob.error = TRUE
num.threads = NULL; save.memory = FALSE
verbose = FALSE; seed = NULL
dependent.variable.name = NULL; status.variable.name = NULL
classification = NULL; nsplits = 5; proptry = 1

  ## GenABEL GWA data
  if ("gwaa.data" %in% class(data)) {
    snp.names <- data@gtdata@snpnames
    snp.data <- data@gtdata@gtps@.Data
    data <- data@phdata
    if ("id" %in% names(data)) {
      data$"id" <- NULL
    }
    gwa.mode <- TRUE
    save.memory <- FALSE
  } else {
    snp.data <- as.matrix(0)
    gwa.mode <- FALSE
  }

  ## Sparse matrix data
  if (inherits(data, "Matrix")) {
    if (!("dgCMatrix" %in% class(data))) {
      stop("Error: Currently only sparse data of class 'dgCMatrix' supported.")
    }

    if (!is.null(formula)) {
      stop("Error: Sparse matrices only supported with alternative interface. Use dependent.variable.name instead of formula.")
    }
  }

  ## Formula interface. Use whole data frame is no formula provided and depvarname given
  if (is.null(formula)) {
    if (is.null(dependent.variable.name)) {
      stop("Error: Please give formula or dependent variable name.")
    }
    if (is.null(status.variable.name)) {
      status.variable.name <- ""
      response <- data[, dependent.variable.name, drop = TRUE]
    } else {
      response <- survival::Surv(data[, dependent.variable.name], data[, status.variable.name]) #data[, c(dependent.variable.name, status.variable.name)]
    }
    data.selected <- data
  } else {
    formula <- formula(formula)
    if (class(formula) != "formula") {
      stop("Error: Invalid formula.")
    }
    data.selected <- parse.formula(formula, data, env = parent.frame())
    response <- data.selected[, 1]
  }

  ## Check missing values
  if (any(is.na(data.selected))) {
    offending_columns <- colnames(data.selected)[colSums(is.na(data.selected)) > 0]
    stop("Missing data in columns: ",
         paste0(offending_columns, collapse = ", "), ".", call. = FALSE)
  }

  ## Check response levels
  if (is.factor(response)) {
    if (nlevels(response) != nlevels(droplevels(response))) {
      dropped_levels <- setdiff(levels(response), levels(droplevels(response)))
      warning("Dropped unused factor level(s) in dependent variable: ",
              paste0(dropped_levels, collapse = ", "), ".", call. = FALSE)
    }
  }

  ## Treetype
  if (is.factor(response)) {
    if (probability) {
      treetype <- 9
    } else {
      treetype <- 1
    }
  } else if (is.numeric(response) && (is.null(ncol(response)) || ncol(response) == 1)) {
    if (!is.null(classification) && classification && !probability) {
      treetype <- 1
    } else if (probability) {
      treetype <- 9
    } else {
      treetype <- 3
    }
  } else if (class(response) == "Surv" || is.data.frame(response) || is.matrix(response)) {
    treetype <- 5
  } else {
    stop("Error: Unsupported type of dependent variable.")
  }

  ## Quantile prediction only for regression
  if (quantreg && treetype != 3) {
    stop("Error: Quantile prediction implemented only for regression outcomes.")
  }

  ## Dependent and status variable name. For non-survival dummy status variable name.
  if (!is.null(formula)) {
    if (treetype == 5) {
      dependent.variable.name <- dimnames(response)[[2]][1]
      status.variable.name <- dimnames(response)[[2]][2]
    } else {
      dependent.variable.name <- names(data.selected)[1]
      status.variable.name <- ""
    }
    independent.variable.names <- names(data.selected)[-1]
  } else {
    independent.variable.names <- colnames(data.selected)[colnames(data.selected) != dependent.variable.name &
                                                            colnames(data.selected) != status.variable.name]
  }

  ## respect.unordered.factors
  if (is.null(respect.unordered.factors)) {
    if (!is.null(splitrule) && splitrule == "extratrees") {
      respect.unordered.factors <- "partition"
    } else {
      respect.unordered.factors <- "ignore"
    }
  }
## --> respect.unordered.factors <- "ignore"


  ## Old version of respect.unordered.factors
  if (respect.unordered.factors == TRUE) {
    respect.unordered.factors <- "order"
  } else if (respect.unordered.factors == FALSE) {
    respect.unordered.factors <- "ignore"
  }

  ## Recode characters as factors and recode factors if 'order' mode
  if (!is.matrix(data.selected) && !inherits(data.selected, "Matrix")) {
    character.idx <- sapply(data.selected, is.character)

    if (respect.unordered.factors == "order") {
      ## Recode characters and unordered factors
      names.selected <- names(data.selected)
      ordered.idx <- sapply(data.selected, is.ordered)
      factor.idx <- sapply(data.selected, is.factor)
      independent.idx <- names.selected != dependent.variable.name &
        names.selected != status.variable.name &
        names.selected != paste0("Surv(", dependent.variable.name, ", ", status.variable.name, ")")
      recode.idx <- independent.idx & (character.idx | (factor.idx & !ordered.idx))

      if (any(recode.idx) & (importance == "impurity_corrected" || importance == "impurity_unbiased")) {
        warning("Corrected impurity importance may not be unbiased for re-ordered factor levels. Consider setting respect.unordered.factors to 'ignore' or 'partition' or manually compute corrected importance.")
      }

      ## Numeric response
      if (is.factor(response)) {
        num.response <- as.numeric(response)
      } else {
        num.response <- response
      }

      ## Recode each column
      data.selected[recode.idx] <- lapply(data.selected[recode.idx], function(x) {
        if (!is.factor(x)) {
          x <- as.factor(x)
        }

        if ("Surv" %in% class(response)) {
          ## Use median survival if available or largest quantile available in all strata if median not available
          levels.ordered <- largest.quantile(response ~ x)

          ## Get all levels not in node
          levels.missing <- setdiff(levels(x), levels.ordered)
          levels.ordered <- c(levels.missing, levels.ordered)
        } else if (is.factor(response) & nlevels(response) > 2) {
          levels.ordered <- pca.order(y = response, x = x)
        } else {
          ## Order factor levels by mean response
          means <- sapply(levels(x), function(y) {
            mean(num.response[x == y])
          })
          levels.ordered <- as.character(levels(x)[order(means)])
        }

        ## Return reordered factor
        factor(x, levels = levels.ordered, ordered = TRUE, exclude = NULL)
      })

      ## Save levels
      covariate.levels <- lapply(data.selected[independent.idx], levels)
    } else {
      ## Recode characters only
      data.selected[character.idx] <- lapply(data.selected[character.idx], factor)
    }
  }

  ## Input data and variable names, create final data matrix
  if (!is.null(formula) && treetype == 5) {
    data.final <- data.matrix(cbind(response[, 1], response[, 2],
                                    data.selected[-1]))
    colnames(data.final) <- c(dependent.variable.name, status.variable.name,
                              independent.variable.names)
  } else if (is.matrix(data.selected) || inherits(data.selected, "Matrix")) {
    data.final <- data.selected
  } else {
    data.final <- data.matrix(data.selected)
  }
  variable.names <- colnames(data.final)

  ## If gwa mode, add snp variable names
  if (gwa.mode) {
    variable.names <- c(variable.names, snp.names)
    all.independent.variable.names <- c(independent.variable.names, snp.names)
  } else {
    all.independent.variable.names <- independent.variable.names
  }

  ## Error if no covariates
  if (length(all.independent.variable.names) < 1) {
    stop("Error: No covariates found.")
  }

  ## Number of trees
  if (!is.numeric(num.trees) || num.trees < 1) {
    stop("Error: Invalid value for num.trees.")
  }

  ## mtry
  if (is.null(mtry)) {
    mtry <- 0
  } else if (!is.numeric(mtry) || mtry < 0) {
    stop("Error: Invalid value for mtry")
  }

  ## proptry ## asdf
  if (is.null(proptry)) { ## asdf
    proptry <- 0 ## asdf
  } else if (!is.numeric(proptry) || !((proptry > 0) & (proptry <= 1))) { ## asdf
    stop("Error: Invalid value for proptry") ## asdf
  } ## asdf

  ## Seed
  if (is.null(seed)) {
    seed <- runif(1 , 0, .Machine$integer.max)
  }

  ## Keep inbag
  if (!is.logical(keep.inbag)) {
    stop("Error: Invalid value for keep.inbag")
  }

  ## Num threads
  ## Default 0 -> detect from system in C++.
  if (is.null(num.threads)) {
    num.threads = 0
  } else if (!is.numeric(num.threads) || num.threads < 0) {
    stop("Error: Invalid value for num.threads")
  }

  ## Minumum node size
  if (is.null(min.node.size)) {
    min.node.size <- 0
  } else if (!is.numeric(min.node.size) || min.node.size < 0) {
    stop("Error: Invalid value for min.node.size")
  }

  ## Tree depth
  if (is.null(max.depth)) {
    max.depth <- 0
  } else if (!is.numeric(max.depth) || max.depth < 0) {
    stop("Error: Invalid value for max.depth. Please give a positive integer.")
  }

  ## Sample fraction
  if (!is.numeric(sample.fraction)) {
    stop("Error: Invalid value for sample.fraction. Please give a value in (0,1] or a vector of values in [0,1].")
  }
  if (length(sample.fraction) > 1) {
    if (!(treetype %in% c(1, 9))) {
      stop("Error: Invalid value for sample.fraction. Vector values only valid for classification forests.")
    }
    if (any(sample.fraction < 0) || any(sample.fraction > 1)) {
      stop("Error: Invalid value for sample.fraction. Please give a value in (0,1] or a vector of values in [0,1].")
    }
    if (sum(sample.fraction) <= 0) {
      stop("Error: Invalid value for sample.fraction. Sum of values must be >0.")
    }
    if (length(sample.fraction) != nlevels(response)) {
      stop("Error: Invalid value for sample.fraction. Expecting ", nlevels(response), " values, provided ", length(sample.fraction), ".")
    }
    if (!replace & any(sample.fraction * length(response) > table(response))) {
      idx <- which(sample.fraction * length(response) > table(response))[1]
      stop("Error: Not enough samples in class ", names(idx),
           "; available: ", table(response)[idx],
           ", requested: ", (sample.fraction * length(response))[idx], ".")
    }
    if (!is.null(case.weights)) {
      stop("Error: Combination of case.weights and class-wise sampling not supported.")
    }
  } else {
    if (sample.fraction <= 0 || sample.fraction > 1) {
      stop("Error: Invalid value for sample.fraction. Please give a value in (0,1] or a vector of values in [0,1].")
    }
  }

  ## Importance mode
  if (is.null(importance) || importance == "none") {
    importance.mode <- 0
  } else if (importance == "impurity") {
    importance.mode <- 1
  } else if (importance == "impurity_corrected" || importance == "impurity_unbiased") {
    importance.mode <- 5
  } else if (importance == "permutation") {
    if (scale.permutation.importance) {
      importance.mode <- 2
    } else {
      importance.mode <- 3
    }
  } else {
    stop("Error: Unknown importance mode.")
  }

  ## Case weights: NULL for no weights
  if (is.null(case.weights)) {
    case.weights <- c(0,0)
    use.case.weights <- FALSE
    if (holdout) {
      stop("Error: Case weights required to use holdout mode.")
    }
  } else {
    use.case.weights <- TRUE

    ## Sample from non-zero weights in holdout mode
    if (holdout) {
      sample.fraction <- sample.fraction * mean(case.weights > 0)
    }

    if (!replace && sum(case.weights > 0) < sample.fraction * nrow(data.final)) {
      stop("Error: Fewer non-zero case weights than observations to sample.")
    }
  }

  ## Manual inbag selection
  if (is.null(inbag)) {
    inbag <- list(c(0,0))
    use.inbag <- FALSE
  } else if (is.list(inbag)) {
    use.inbag <- TRUE
    if (use.case.weights) {
      stop("Error: Combination of case.weights and inbag not supported.")
    }
    if (length(sample.fraction) > 1) {
      stop("Error: Combination of class-wise sampling and inbag not supported.")
    }
    if (length(inbag) != num.trees) {
      stop("Error: Size of inbag list not equal to number of trees.")
    }
  } else {
    stop("Error: Invalid inbag, expects list of vectors of size num.trees.")
  }

  ## Class weights: NULL for no weights (all 1)
  if (is.null(class.weights)) {
    class.weights <- rep(1, nlevels(response))
  } else {
    if (!(treetype %in% c(1, 9))) {
      stop("Error: Argument class.weights only valid for classification forests.")
    }
    if (!is.numeric(class.weights) || any(class.weights < 0)) {
      stop("Error: Invalid value for class.weights. Please give a vector of non-negative values.")
    }
    if (length(class.weights) != nlevels(response)) {
      stop("Error: Number of class weights not equal to number of classes.")
    }

    ## Reorder (C++ expects order as appearing in the data)
    class.weights <- class.weights[unique(as.numeric(response))]
  }

  ## Split select weights: NULL for no weights
  if (is.null(split.select.weights)) {
    split.select.weights <- list(c(0,0))
    use.split.select.weights <- FALSE
  } else if (is.numeric(split.select.weights)) {
    if (length(split.select.weights) != length(all.independent.variable.names)) {
      stop("Error: Number of split select weights not equal to number of independent variables.")
    }
    split.select.weights <- list(split.select.weights)
    use.split.select.weights <- TRUE
  } else if (is.list(split.select.weights)) {
    if (length(split.select.weights) != num.trees) {
      stop("Error: Size of split select weights list not equal to number of trees.")
    }
    use.split.select.weights <- TRUE
  } else {
    stop("Error: Invalid split select weights.")
  }

  ## Always split variables: NULL for no variables
  if (is.null(always.split.variables)) {
    always.split.variables <- c("0", "0")
    use.always.split.variables <- FALSE
  } else {
    use.always.split.variables <- TRUE
  }

  if (use.split.select.weights && use.always.split.variables) {
    stop("Error: Please use only one option of split.select.weights and always.split.variables.")
  }

  ## Splitting rule
  if (is.null(splitrule)) {
    if (treetype == 5) {
      splitrule <- "logrank"
    } else if (treetype == 3) {
      splitrule <- "variance"
    } else if (treetype %in% c(1, 9)) {
      splitrule <- "gini"
    }
    splitrule.num <- 1
  } else if (splitrule == "logrank") {
    if (treetype == 5) {
      splitrule.num <- 1
    } else {
      stop("Error: logrank splitrule applicable to survival data only.")
    }
  } else if (splitrule == "gini") {
    if (treetype %in% c(1, 9)) {
      splitrule.num <- 1
    } else {
      stop("Error: Gini splitrule applicable to classification data only.")
    }
  } else if (splitrule == "variance") {
    if (treetype == 3) {
      splitrule.num <- 1
    } else {
      stop("Error: variance splitrule applicable to regression data only.")
    }
  } else if (splitrule == "auc" || splitrule == "C") {
    if (treetype == 5) {
      splitrule.num <- 2
    } else {
      stop("Error: C index splitrule applicable to survival data only.")
    }
  } else if (splitrule == "auc_ignore_ties" || splitrule == "C_ignore_ties") {
    if (treetype == 5) {
      splitrule.num <- 3
    } else {
      stop("Error: C index splitrule applicable to survival data only.")
    }
  } else if (splitrule == "maxstat") {
    if (treetype == 5 || treetype == 3) {
      splitrule.num <- 4
    } else {
      stop("Error: maxstat splitrule applicable to regression or survival data only.")
    }
  } else if (splitrule == "extratrees") {
    splitrule.num <- 5
  } else {
    stop("Error: Unknown splitrule.")
  }

  ## Maxstat splitting
  if (alpha < 0 || alpha > 1) {
    stop("Error: Invalid value for alpha, please give a value between 0 and 1.")
  }
  if (minprop < 0 || minprop > 0.5) {
    stop("Error: Invalid value for minprop, please give a value between 0 and 0.5.")
  }

  ## Extra trees
  if (!is.numeric(num.random.splits) || num.random.splits < 1) {
    stop("Error: Invalid value for num.random.splits, please give a positive integer.")
  }
  if (splitrule.num == 5 && save.memory && respect.unordered.factors == "partition") {
    stop("Error: save.memory option not possible in extraTrees mode with unordered predictors.")
  }

  ## Unordered factors
  if (respect.unordered.factors == "partition") {
    names.selected <- names(data.selected)
    ordered.idx <- sapply(data.selected, is.ordered)
    factor.idx <- sapply(data.selected, is.factor)
    independent.idx <- names.selected != dependent.variable.name & names.selected != status.variable.name
    unordered.factor.variables <- names.selected[factor.idx & !ordered.idx & independent.idx]

    if (length(unordered.factor.variables) > 0) {
      use.unordered.factor.variables <- TRUE
      ## Check level count
      num.levels <- sapply(data.selected[, factor.idx & !ordered.idx & independent.idx, drop = FALSE], nlevels)
      max.level.count <- .Machine$double.digits
      if (max(num.levels) > max.level.count) {
        stop(paste("Too many levels in unordered categorical variable ", unordered.factor.variables[which.max(num.levels)],
                   ". Only ", max.level.count, " levels allowed on this system. Consider using the 'order' option.", sep = ""))
      }
    } else {
      unordered.factor.variables <- c("0", "0")
      use.unordered.factor.variables <- FALSE
    }
  } else if (respect.unordered.factors == "ignore" || respect.unordered.factors == "order") {
    ## Ordering for "order" is handled above
    unordered.factor.variables <- c("0", "0")
    use.unordered.factor.variables <- FALSE
  } else {
    stop("Error: Invalid value for respect.unordered.factors, please use 'order', 'partition' or 'ignore'.")
  }

  ## Unordered maxstat splitting not possible
  if (use.unordered.factor.variables && !is.null(splitrule)) {
    if (splitrule == "maxstat") {
      stop("Error: Unordered factor splitting not implemented for 'maxstat' splitting rule.")
    } else if (splitrule %in% c("C", "auc", "C_ignore_ties", "auc_ignore_ties")) {
      stop("Error: Unordered factor splitting not implemented for 'C' splitting rule.")
    }
  }

  ## Warning for experimental 'order' splitting
  if (respect.unordered.factors == "order") {
    if (treetype == 3 && splitrule == "maxstat") {
      warning("Warning: The 'order' mode for unordered factor handling with the 'maxstat' splitrule is experimental.")
    }
    if (gwa.mode & ((treetype %in% c(1,9) & nlevels(response) > 2) | treetype == 5)) {
      stop("Error: Ordering of SNPs currently only implemented for regression and binary outcomes.")
    }
  }


  ## Prediction mode always false. Use predict.divfor() method.
  prediction.mode <- FALSE
  predict.all <- FALSE
  prediction.type <- 1

  ## No loaded forest object
  loaded.forest <- list()

  ## Use sparse matrix
  if ("dgCMatrix" %in% class(data.final)) {
    sparse.data <- data.final
    data.final <- matrix(c(0, 0))
    use.sparse.data <- TRUE
  } else {
    sparse.data <- Matrix(matrix(c(0, 0)))
    use.sparse.data <- FALSE
  }

  if (respect.unordered.factors == "order"){
    order.snps <- TRUE
  } else {
    order.snps <- FALSE
  }


  head(data.final)

  data.final[,2]



  dependent.variable.name

  head(data.final)
  table(data.final[,3])
  table(veteranclass$celltype)
  levels(veteranclass$celltype)

  sapply(levels(veteranclass$celltype), function(x) mean(veteranclass$Y[veteranclass$celltype==x]==1))
  sort(sapply(levels(veteranclass$celltype), function(x) mean(veteranclass$Y[veteranclass$celltype==x]==1)))
  as.numeric(factor(names(sort(sapply(levels(veteranclass$celltype), function(x) mean(veteranclass$Y[veteranclass$celltype==x]==1)))), levels=levels(veteranclass$celltype)))

  table(data.final[,3], veteranclass$celltype)

  # data.finalignore <- data.final


  ## Clean up
  rm("data.selected")

  ## Call divfor
  result <- divforCpp(treetype, dependent.variable.name, data.final, variable.names, mtry,
                      num.trees, verbose, seed, num.threads, write.forest, importance.mode,
                      min.node.size, split.select.weights, use.split.select.weights,
                      always.split.variables, use.always.split.variables,
                      status.variable.name, prediction.mode, loaded.forest, snp.data,
                      replace, probability, unordered.factor.variables, use.unordered.factor.variables,
                      save.memory, splitrule.num, case.weights, use.case.weights, class.weights,
                      predict.all, keep.inbag, sample.fraction, alpha, minprop, holdout, prediction.type,
                      num.random.splits, sparse.data, use.sparse.data, order.snps, oob.error, max.depth,
                      inbag, use.inbag, nsplits, proptry) ## asdf

  if (length(result) == 0) {
    stop("User interrupt or internal error.")
  }

  ## Prepare results
  if (importance.mode != 0) {
    names(result$variable.importance) <- all.independent.variable.names
  }

  ## Set predictions
  if (treetype == 1 && is.factor(response) && oob.error) {
    result$predictions <- integer.to.factor(result$predictions,
                                            levels(response))
    true.values <- integer.to.factor(unlist(data.final[, dependent.variable.name]),
                                     levels(response))
    result$confusion.matrix <- table(true.values, result$predictions,
                                     dnn = c("true", "predicted"), useNA = "ifany")
  } else if (treetype == 5 && oob.error) {
    if (is.list(result$predictions)) {
      result$predictions <- do.call(rbind, result$predictions)
    }
    if (is.vector(result$predictions)) {
      result$predictions <- matrix(result$predictions, nrow = 1)
    }
    result$chf <- result$predictions
    result$predictions <- NULL
    result$survival <- exp(-result$chf)
  } else if (treetype == 9 && !is.matrix(data) && oob.error) {
    if (is.list(result$predictions)) {
      result$predictions <- do.call(rbind, result$predictions)
    }
    if (is.vector(result$predictions)) {
      result$predictions <- matrix(result$predictions, nrow = 1)
    }

    ## Set colnames and sort by levels
    colnames(result$predictions) <- unique(response)
    if (is.factor(response)) {
      result$predictions <- result$predictions[, levels(droplevels(response)), drop = FALSE]
    }
  }

  ## Splitrule
  result$splitrule <- splitrule

  ## Set treetype
  if (treetype == 1) {
    result$treetype <- "Classification"
  } else if (treetype == 3) {
    result$treetype <- "Regression"
  } else if (treetype == 5) {
    result$treetype <- "Survival"
  } else if (treetype == 9) {
    result$treetype <- "Probability estimation"
  }
  if (treetype == 3) {
    result$r.squared <- 1 - result$prediction.error / var(response)
  }
  result$call <- sys.call()
  result$importance.mode <- importance
  result$num.samples <- nrow(data.final)
  result$replace <- replace

  ## Write forest object
  if (write.forest) {
    if (is.factor(response)) {
      result$forest$levels <- levels(response)
    }
    result$forest$independent.variable.names <- independent.variable.names
    result$forest$treetype <- result$treetype
    class(result$forest) <- "divfor.forest"

    ## In 'ordered' mode, save covariate levels
    if (respect.unordered.factors == "order" && !is.matrix(data)) {
      result$forest$covariate.levels <- covariate.levels
    }
  }

  class(result) <- "divfor"

  ## Prepare quantile prediction
  if (quantreg) {
    terminal.nodes <- predict(result, data, type = "terminalNodes")$predictions + 1
    n <- result$num.samples
    result$random.node.values <- matrix(nrow = max(terminal.nodes), ncol = num.trees)

    ## Select one random obs per node and tree
    for (tree in 1:num.trees){
      idx <- sample(1:n, n)
      result$random.node.values[terminal.nodes[idx, tree], tree] <- response[idx]
    }

    ## Prepare out-of-bag quantile regression
    if(!is.null(result$inbag.counts)) {
      inbag.counts <- simplify2array(result$inbag.counts)
      random.node.values.oob <- 0 * terminal.nodes
      random.node.values.oob[inbag.counts > 0] <- NA

      ## For each tree and observation select one random obs in the same node (not the same obs)
      for (tree in 1:num.trees){
        is.oob <- inbag.counts[, tree] == 0
        num.oob <- sum(is.oob)

        if (num.oob != 0) {
          oob.obs <- which(is.oob)
          oob.nodes <- terminal.nodes[oob.obs, tree]
          for (j in 1:num.oob) {
            idx <- terminal.nodes[, tree] == oob.nodes[j]
            idx[oob.obs[j]] <- FALSE
            random.node.values.oob[oob.obs[j], tree] <- save.sample(response[idx], size = 1)
          }
        }
      }

      ## Check num.trees
      minoob <- min(rowSums(inbag.counts == 0))
      if (minoob < 10) {
        stop("Error: Too few trees for out-of-bag quantile regression.")
      }

      ## Use the same number of values for all obs, select randomly
      result$random.node.values.oob <- t(apply(random.node.values.oob, 1, function(x) {
        sample(x[!is.na(x)], minoob)
      }))
    }
  }

  result$nsplits <- nsplits
  result$proptry <- proptry

  result$mtry <- NULL
  result$max.triedsplits <- NULL

  return(result)
}






